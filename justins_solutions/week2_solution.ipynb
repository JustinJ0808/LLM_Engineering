{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56531bb3",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "739cf697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c23ccfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OpenAI client\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "else:\n",
    "    print(\"OpenAI API key loaded successfully\")\n",
    "\n",
    "MODEL = \"gpt-4.1-mini\"\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41f50ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an expert assistant.\n",
    "You explain things in a way that is easy to understand.\n",
    "You are like a teacher or professor.\n",
    "Always be accurate. If you don't know the answer, say so.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36cf31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history, model):\n",
    "   messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "   for h in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": h[0]})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": h[1]})\n",
    "\n",
    "   messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "   stream = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "   response = \"\"\n",
    "   for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            response += chunk.choices[0].delta.content\n",
    "            yield response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e70a7451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "with gr.Blocks(fill_height=True) as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– AI Technical Assistant\")\n",
    "    \n",
    "    # Place it at the top to clear the path for examples\n",
    "    with gr.Row():\n",
    "        model_dropdown = gr.Dropdown(\n",
    "            choices=[\"gpt-4o\", \"gpt-4o-mini\", \"o1-preview\"],\n",
    "            value=\"gpt-4o-mini\",\n",
    "            label=\"Brain Selection\"\n",
    "        )\n",
    "    \n",
    "    # The ChatInterface will now see 'model_dropdown' is already rendered \n",
    "    # and won't put it in the way of the examples.\n",
    "    gr.ChatInterface(\n",
    "        fn=chat,\n",
    "        additional_inputs=[model_dropdown],\n",
    "        examples=[\n",
    "            [\"explain what is a large language model\"],\n",
    "            [\"what are the benefits of using a large language model?\"],\n",
    "            [\"what are the drawbacks of using a large language model?\"],\n",
    "            [\"what are some common use cases for large language models?\"],\n",
    "            [\"what are some common pitfalls to avoid when using large language models?\"],\n",
    "        ]\n",
    "    )\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
